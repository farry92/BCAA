import streamlit as st
import pandas as pd
import time
import os
import sqlite3
import json
from datetime import datetime
from config import Config
from medical_processor import MedicalProcessor

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="åŒ»å­¦ç—‡çŠ¶æå–ç³»ç»Ÿ",
    page_icon="ğŸ¥",
    layout="wide"
)

# æŸ¥è¯¢è®°å½•ç®¡ç†ç±»
class QueryRecorder:
    """æŸ¥è¯¢è®°å½•ç®¡ç†å™¨ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ æ•°æ®æ”¶é›†"""
    
    def __init__(self, records_file="medical_query_records.jsonl"):
        self.records_file = records_file
        self.ensure_file_exists()
    
    def ensure_file_exists(self):
        """ç¡®ä¿è®°å½•æ–‡ä»¶å­˜åœ¨"""
        if not os.path.exists(self.records_file):
            with open(self.records_file, 'w', encoding='utf-8') as f:
                pass  # åˆ›å»ºç©ºæ–‡ä»¶
    
    def save_query_record(self, user_input, question_type, task_type, result, processing_time):
        """ä¿å­˜æŸ¥è¯¢è®°å½•å¹¶è¿”å›è®°å½•ID"""
        record_id = f"query_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        
        record = {
            "record_id": record_id,
            "timestamp": datetime.now().isoformat(),
            "query_info": {
                "user_input": user_input,
                "question_type": question_type,
                "task_type": task_type,
                "processing_time": processing_time
            },
            "system_response": {
                "main_kb_answer": result.get('main_kb_answer', ''),
                "symptom_kb_answer": result.get('symptom_kb_answer', ''),
                "overall_answer": result.get('overall_answer', ''),
                "overall_status": result.get('overall_status', 'unknown'),
                "main_kb_has_reference": result.get('main_kb_has_reference', False),
                "symptom_kb_has_reference": result.get('symptom_kb_has_reference', False),
                "main_kb_final_retrieved_count": result.get('main_kb_final_retrieved_count', 0),
                "symptom_kb_final_retrieved_count": result.get('symptom_kb_final_retrieved_count', 0),
                "main_kb_processing_time": result.get('main_kb_processing_time', 0),
                "symptom_kb_processing_time": result.get('symptom_kb_processing_time', 0),
                # æ·»åŠ ç—‡çŠ¶æå–ç»“æœ
                "extracted_symptoms": result.get('overall_pure_symptoms', ''),
                "symptom_consistent": result.get('is_symptom_consistent', ''),
                "total_processing_time": result.get('total_processing_time', 0)
            },
            "user_feedback": None,
            "reinforcement_learning": {
                "reward_signal": None,
                "quality_score": None,
                "retrieval_effectiveness": None,
                "user_satisfaction": None
            }
        }
        
        # ä¿å­˜è®°å½•
        with open(self.records_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        return record_id
    
    def update_feedback(self, record_id, feedback_text, rating, detailed_feedback=None):
        """æ›´æ–°ç”¨æˆ·åé¦ˆå¹¶è®¡ç®—å¼ºåŒ–å­¦ä¹ ä¿¡å·"""
        records = self.load_all_records()
        updated = False
        
        for i, record in enumerate(records):
            if record.get('record_id') == record_id:
                # æ›´æ–°åé¦ˆ
                record['user_feedback'] = {
                    "feedback_text": feedback_text,
                    "rating": rating,
                    "detailed_feedback": detailed_feedback or {},
                    "feedback_timestamp": datetime.now().isoformat()
                }
                
                # è®¡ç®—å¼ºåŒ–å­¦ä¹ ä¿¡å·
                reward_signal = self._calculate_reward_signal(record, rating)
                quality_score = self._calculate_quality_score(record, rating, feedback_text)
                retrieval_effectiveness = self._calculate_retrieval_effectiveness(record, rating)
                
                record['reinforcement_learning'] = {
                    "reward_signal": reward_signal,
                    "quality_score": quality_score,
                    "retrieval_effectiveness": retrieval_effectiveness,
                    "user_satisfaction": rating / 5.0
                }
                
                records[i] = record # æ›´æ–°åˆ—è¡¨ä¸­çš„è®°å½•
                updated = True
                break
        
        if updated:
            # é‡å†™æ•´ä¸ªæ–‡ä»¶
            with open(self.records_file, 'w', encoding='utf-8') as f:
                for r in records:
                    f.write(json.dumps(r, ensure_ascii=False) + '\n')
            return True
        return False
    
    def _calculate_reward_signal(self, record, rating):
        """è®¡ç®—å¥–åŠ±ä¿¡å· (-1 åˆ° 1 ä¹‹é—´)"""
        # åŸºç¡€å¥–åŠ±åŸºäºç”¨æˆ·è¯„åˆ†
        base_reward = (rating - 3) / 2.0
        
        # æ ¹æ®ç³»ç»Ÿæ€§èƒ½è°ƒæ•´
        performance_bonus = 0
        if record['system_response']['main_kb_has_reference']:
            performance_bonus += 0.1
        if record['system_response']['symptom_kb_has_reference']:
            performance_bonus += 0.1
        
        # å¤„ç†æ—¶é—´æƒ©ç½š
        processing_time = record['query_info']['processing_time']
        time_penalty = max(0, (processing_time - 5) * 0.02)  # è¶…è¿‡5ç§’å¼€å§‹æƒ©ç½š
        
        return min(1.0, max(-1.0, base_reward + performance_bonus - time_penalty))
    
    def _calculate_quality_score(self, record, rating, feedback_text):
        """è®¡ç®—å“åº”è´¨é‡åˆ†æ•°"""
        quality = rating / 5.0
        
        # æ–‡æœ¬æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        positive_keywords = ['å‡†ç¡®', 'æœ‰ç”¨', 'è¯¦ç»†', 'æ¸…æ¥š', 'æ»¡æ„', 'å¥½', 'ä¸é”™', 'å®Œæ•´']
        negative_keywords = ['é”™è¯¯', 'ä¸å‡†ç¡®', 'æ— å…³', 'ä¸æ¸…æ¥š', 'ä¸æ»¡æ„', 'å·®', 'æ— ç”¨', 'ä¸å®Œæ•´']
        
        positive_count = sum(1 for word in positive_keywords if word in feedback_text)
        negative_count = sum(1 for word in negative_keywords if word in feedback_text)
        
        if positive_count + negative_count > 0:
            sentiment_score = positive_count / (positive_count + negative_count)
            quality = (quality + sentiment_score) / 2
        
        return quality
    
    def _calculate_retrieval_effectiveness(self, record, rating):
        """è®¡ç®—æ£€ç´¢æœ‰æ•ˆæ€§"""
        main_effective = record['system_response']['main_kb_has_reference']
        symptom_effective = record['system_response']['symptom_kb_has_reference']
        
        effectiveness = 0
        if main_effective and symptom_effective:
            effectiveness = rating / 5.0
        elif main_effective or symptom_effective:
            effectiveness = (rating / 5.0) * 0.7
        else:
            effectiveness = (rating / 5.0) * 0.3
        
        return effectiveness
    
    def load_all_records(self):
        """åŠ è½½æ‰€æœ‰è®°å½•"""
        records = []
        try:
            with open(self.records_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        records.append(json.loads(line))
        except FileNotFoundError:
            pass
        return records
    
    def get_rl_training_data(self):
        """è·å–å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®"""
        records = self.load_all_records()
        return [r for r in records if r.get('user_feedback') and r.get('reinforcement_learning', {}).get('reward_signal') is not None]
    
    def export_rl_data(self, output_file="rl_training_data.json"):
        """å¯¼å‡ºå¼ºåŒ–å­¦ä¹ æ•°æ®"""
        rl_data = self.get_rl_training_data()
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(rl_data, f, ensure_ascii=False, indent=2)
        return len(rl_data)

# åˆå§‹åŒ–æŸ¥è¯¢è®°å½•å™¨
# ç§»é™¤ @st.cache_resource è£…é¥°å™¨ï¼Œç¡®ä¿æ¯æ¬¡éƒ½èƒ½è·å–æœ€æ–°å®ä¾‹
def get_query_recorder_instance():
    return QueryRecorder()

recorder = get_query_recorder_instance()

# æ•°æ®åº“åˆå§‹åŒ–ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
def init_database():
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
    conn = sqlite3.connect('query_logs.db')
    cursor = conn.cursor()
    
    # åˆ›å»ºæŸ¥è¯¢è®°å½•è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS query_logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            user_input TEXT NOT NULL,
            question_type TEXT NOT NULL,
            task_type TEXT NOT NULL,
            result_data TEXT NOT NULL,
            processing_time REAL,
            status TEXT,
            main_kb_used BOOLEAN,
            symptom_kb_used BOOLEAN,
            user_feedback TEXT,
            feedback_rating INTEGER,
            feedback_timestamp TEXT
        )
    ''')
    
    # æ£€æŸ¥å¹¶æ·»åŠ æ–°åˆ—
    cursor.execute("PRAGMA table_info(query_logs)")
    columns = [column[1] for column in cursor.fetchall()]
    
    new_columns_with_types = {
        'record_id': 'TEXT',
        'extracted_symptoms': 'TEXT',
        'symptom_consistent': 'TEXT'
    }
    
    for col_name, col_type in new_columns_with_types.items():
        if col_name not in columns:
            try:
                cursor.execute(f'ALTER TABLE query_logs ADD COLUMN {col_name} {col_type}')
                print(f"Added column {col_name} to query_logs table.")
            except sqlite3.OperationalError as e:
                print(f"Error adding column {col_name}: {e}") # Debugging
    
    conn.commit()
    conn.close()

def save_query_log(user_input, question_type, task_type, result, processing_time, record_id):
    """ä¿å­˜æŸ¥è¯¢è®°å½•åˆ°æ•°æ®åº“"""
    conn = sqlite3.connect('query_logs.db')
    cursor = conn.cursor()
    
    # ä»resultè·å–extracted_symptomså’Œsymptom_consistent
    extracted_symptoms = result.get('overall_pure_symptoms', '')
    symptom_consistent = result.get('is_symptom_consistent', '')

    try:
        cursor.execute('''
            INSERT INTO query_logs 
            (timestamp, user_input, question_type, task_type, result_data, processing_time, status, 
             main_kb_used, symptom_kb_used, record_id, extracted_symptoms, symptom_consistent)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            datetime.now().isoformat(),
            user_input,
            question_type,
            task_type,
            json.dumps(result, ensure_ascii=False),
            processing_time,
            result.get('overall_status', 'unknown'),
            result.get('main_kb_has_reference', False),
            result.get('symptom_kb_has_reference', False),
            record_id,
            extracted_symptoms,
            symptom_consistent
        ))
    except sqlite3.OperationalError as e:
        print(f"æ•°æ®åº“æ’å…¥å¤±è´¥ï¼Œå¯èƒ½ç¼ºå°‘åˆ—æˆ–æ•°æ®ç±»å‹ä¸åŒ¹é…: {e}") # Debugging
        # å¦‚æœæŸäº›åˆ—ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºæœ¬æ’å…¥
        cursor.execute('''
            INSERT INTO query_logs 
            (timestamp, user_input, question_type, task_type, result_data, processing_time, status, main_kb_used, symptom_kb_used)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            datetime.now().isoformat(),
            user_input,
            question_type,
            task_type,
            json.dumps(result, ensure_ascii=False),
            processing_time,
            result.get('overall_status', 'unknown'),
            result.get('main_kb_has_reference', False),
            result.get('symptom_kb_has_reference', False)
        ))
    
    query_id = cursor.lastrowid
    conn.commit()
    conn.close()
    return query_id

def save_user_feedback(query_id, record_id, feedback_text, rating, detailed_feedback=None):
    """ä¿å­˜ç”¨æˆ·åé¦ˆåˆ°æ•°æ®åº“å’ŒJSONLæ–‡ä»¶"""
    try:
        # æ›´æ–°æ•°æ®åº“
        conn = sqlite3.connect('query_logs.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE query_logs 
            SET user_feedback = ?, feedback_rating = ?, feedback_timestamp = ?
            WHERE id = ?
        ''', (feedback_text, rating, datetime.now().isoformat(), query_id))
        
        rows_affected = cursor.rowcount
        conn.commit()
        conn.close()
        
        print(f"æ•°æ®åº“æ›´æ–°ï¼šæŸ¥è¯¢ID={query_id}, å½±å“è¡Œæ•°={rows_affected}, è¯„åˆ†={rating}, åé¦ˆæ–‡æœ¬é•¿åº¦={len(feedback_text) if feedback_text else 0}") # å¢åŠ è°ƒè¯•è¾“å‡º
        
        # æ›´æ–°JSONLæ–‡ä»¶
        jsonl_success = False
        if record_id and record_id != 'N/A':
            recorder_instance = get_query_recorder_instance() # ç¡®ä¿è·å–åˆ°æœ€æ–°çš„å®ä¾‹
            jsonl_success = recorder_instance.update_feedback(record_id, feedback_text, rating, detailed_feedback)
            print(f"JSONLæ›´æ–°ç»“æœ: {jsonl_success}")
        
        return rows_affected > 0 or jsonl_success
        
    except Exception as e:
        print(f"ä¿å­˜åé¦ˆæ—¶å‡ºé”™: {e}")
        return False

def get_query_logs(limit=50, force_refresh=False):
    """è·å–æŸ¥è¯¢è®°å½•"""
    # ä½¿ç”¨ç¼“å­˜é”®ï¼Œä½†å…è®¸å¼ºåˆ¶åˆ·æ–°
    cache_key = f'query_logs_cache_{limit}'
    
    if not force_refresh and cache_key in st.session_state:
        return st.session_state[cache_key]
    
    conn = sqlite3.connect('query_logs.db')
    cursor = conn.cursor()
    
    # æ£€æŸ¥è¡¨ç»“æ„
    cursor.execute("PRAGMA table_info(query_logs)")
    columns = [column[1] for column in cursor.fetchall()]
    
    # æ„å»ºæŸ¥è¯¢è¯­å¥
    base_columns = "id, timestamp, user_input, question_type, processing_time, status, user_feedback, feedback_rating"
    
    select_columns_list = [base_columns]
    
    if 'record_id' in columns:
        select_columns_list.append("CASE WHEN record_id IS NULL THEN 'N/A' ELSE record_id END as record_id")
    else:
        select_columns_list.append("'N/A' as record_id")
    
    if 'extracted_symptoms' in columns:
        select_columns_list.append("CASE WHEN extracted_symptoms IS NULL THEN '' ELSE extracted_symptoms END as extracted_symptoms")
    else:
        select_columns_list.append("'' as extracted_symptoms")
    
    if 'symptom_consistent' in columns:
        select_columns_list.append("CASE WHEN symptom_consistent IS NULL THEN '' ELSE symptom_consistent END as symptom_consistent")
    else:
        select_columns_list.append("'' as symptom_consistent")
    
    select_clause = ", ".join(select_columns_list)
    
    cursor.execute(f'''
        SELECT {select_clause}
        FROM query_logs 
        ORDER BY timestamp DESC 
        LIMIT ?
    ''', (limit,))
    
    rows = cursor.fetchall()
    conn.close()
    
    # ç¼“å­˜ç»“æœ
    st.session_state[cache_key] = rows
    
    return rows

def export_to_excel():
    """å¯¼å‡ºæŸ¥è¯¢è®°å½•åˆ°Excelæ–‡ä»¶"""
    logs = get_query_logs(1000, force_refresh=True)  # å¼ºåˆ¶åˆ·æ–°è·å–æœ€æ–°æ•°æ®
    
    if not logs:
        return None
    
    # è½¬æ¢ä¸ºDataFrame
    df_data = []
    # å‡è®¾ get_query_logs è¿”å›çš„é¡ºåºæ˜¯ id, timestamp, user_input, question_type, processing_time, status, user_feedback, feedback_rating, record_id, extracted_symptoms, symptom_consistent
    for log in logs:
        df_data.append({
            "ID": log[0],
            "æ—¶é—´": log[1][:19],
            "ç”¨æˆ·è¾“å…¥": log[2],
            "é—®é¢˜ç±»å‹": log[3],
            "å¤„ç†æ—¶é—´(ç§’)": log[4],
            "çŠ¶æ€": log[5],
            "ç”¨æˆ·åé¦ˆ": log[6] if log[6] else "",
            "åé¦ˆè¯„åˆ†": log[7] if log[7] else "",
            "è®°å½•ID": log[8],
            "æå–ç—‡çŠ¶": log[9], # ä»log[9]è·å–
            "ç—‡çŠ¶ä¸€è‡´æ€§": log[10] # ä»log[10]è·å–
        })
    
    df = pd.DataFrame(df_data)
    
    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"medical_query_logs_{timestamp}.xlsx"
    
    # ä¿å­˜Excelæ–‡ä»¶
    df.to_excel(filename, index=False, engine='openpyxl')
    return filename

# åˆå§‹åŒ–æ•°æ®åº“
init_database()

# åˆå§‹åŒ–session state
if 'processor' not in st.session_state:
    st.session_state.processor = None
    st.session_state.initialized = False
if 'current_query_id' not in st.session_state:
    st.session_state.current_query_id = None
if 'current_record_id' not in st.session_state:
    st.session_state.current_record_id = None

def initialize_system(config, multi_shard):
    """åˆå§‹åŒ–åŒ»å­¦å¤„ç†ç³»ç»Ÿ"""
    if st.session_state.processor is None:
        with st.spinner('æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...'):
            try:
                st.write("ğŸ”„ å¼€å§‹åˆå§‹åŒ–...")
                processor = MedicalProcessor(config)
                st.write("âœ… MedicalProcessor åˆ›å»ºæˆåŠŸ")
                
                # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                status_text.text("æ­£åœ¨åˆå§‹åŒ–APIå®¢æˆ·ç«¯...")
                progress_bar.progress(25)
                
                status_text.text("æ­£åœ¨åˆå§‹åŒ–é‡æ’åºæ¨¡å‹...")
                progress_bar.progress(50)
                
                status_text.text(f"æ­£åœ¨åŠ è½½å‘é‡åº“... (å¤šåˆ†ç‰‡æ¨¡å¼: {multi_shard})")
                progress_bar.progress(75)
                
                success = processor.initialize(is_multi_shard=multi_shard)
                
                if success:
                    progress_bar.progress(100)
                    status_text.text("åˆå§‹åŒ–å®Œæˆï¼")
                    
                    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
                    st.write("ğŸ“Š å‘é‡åº“åŠ è½½çŠ¶æ€:")
                    try:
                        # å°è¯•æ£€æµ‹æ˜¯å¦æœ‰æœ‰æ•ˆçš„å‘é‡åº“
                        # è¿™é‡Œéœ€è¦æ ¹æ®vector_managerçš„å®é™…å®ç°æ¥åˆ¤æ–­æ˜¯å¦åŠ è½½æˆåŠŸ
                        # ä¾‹å¦‚ï¼Œå¦‚æœvector_managerç»´æŠ¤ä¸€ä¸ªall_vector_storesåˆ—è¡¨
                        main_loaded = False
                        if hasattr(processor.vector_manager, 'main_vector_store') and processor.vector_manager.main_vector_store is not None:
                             main_loaded = True
                        elif hasattr(processor.vector_manager, 'vector_stores') and len(processor.vector_manager.vector_stores) > 0:
                             main_loaded = True

                        if main_loaded:
                            st.write("âœ… ä¸»çŸ¥è¯†åº“åŠ è½½æˆåŠŸ")
                        else:
                            st.warning("âš ï¸ ä¸»çŸ¥è¯†åº“åŠ è½½å¤±è´¥ï¼ˆè¯·åœ¨æŸ¥è¯¢åè§‚å¯Ÿæ˜¯å¦å®é™…å·¥ä½œæ­£å¸¸ï¼‰") # ä¿®æ­£æç¤º
                    except Exception as e:
                        st.warning(f"âš ï¸ æ— æ³•ç¡®å®šä¸»çŸ¥è¯†åº“çŠ¶æ€ï¼ˆé”™è¯¯: {e}ï¼‰ï¼Œè¯·åœ¨æŸ¥è¯¢åè§‚å¯Ÿæ˜¯å¦å®é™…å·¥ä½œæ­£å¸¸")
                    
                    if hasattr(processor.vector_manager, 'symptom_vector_store') and processor.vector_manager.symptom_vector_store:
                        st.write("âœ… ç—‡çŠ¶çŸ¥è¯†åº“åŠ è½½æˆåŠŸ")
                    else:
                        st.warning("âš ï¸ ç—‡çŠ¶çŸ¥è¯†åº“åŠ è½½å¤±è´¥")
                    
                    st.session_state.processor = processor
                    st.session_state.initialized = True
                    st.write("âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
                    return True
                else:
                    st.error("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
                    return False
                    
            except Exception as e:
                st.error(f"âŒ åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.write(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                import traceback
                st.code(traceback.format_exc())
                return False
    else:
        st.info("âœ… ç³»ç»Ÿå·²ç»åˆå§‹åŒ–")
        return True

def main():
    st.title("ğŸ¥ åŒ»å­¦ç—‡çŠ¶æå–ç³»ç»Ÿ")
    st.markdown("---")
    
    # åˆ›å»ºæ ‡ç­¾é¡µï¼Œå¢åŠ å¼ºåŒ–å­¦ä¹ æ•°æ®æ ‡ç­¾
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ ç—‡çŠ¶æŸ¥è¯¢", "âš™ï¸ ç³»ç»Ÿé…ç½®", "ğŸ“Š æŸ¥è¯¢è®°å½•", "ğŸ¤– å¼ºåŒ–å­¦ä¹ æ•°æ®"])
    
    with tab2:  # ç³»ç»Ÿé…ç½®æ ‡ç­¾é¡µ
        st.header("ç³»ç»Ÿé…ç½®")
        
        # APIé…ç½®
        api_type = st.selectbox(
            "é€‰æ‹©APIç±»å‹", 
            ["deepseek", "doubao", "qianwen", "ernie"]
        )
        
        api_key = st.text_input(
            "APIå¯†é’¥", 
            type="password",
            help="è¾“å…¥æ‚¨çš„APIå¯†é’¥"
        )
        
        # è·¯å¾„é…ç½®
        vector_path = st.text_input(
            "ä¸»å‘é‡åº“è·¯å¾„",
            value="/home/fangyi1/Deepseeké¡¹ç›®/RAGé¡¹ç›®/medical_retrieval/MedSE2RAG/data/vector_database/all_pdfs_vector_db_qwen3/",
            help="ä¸»çŸ¥è¯†åº“çš„å‘é‡å­˜å‚¨è·¯å¾„"
        )
        
        symptom_path = st.text_input(
            "ç—‡çŠ¶å‘é‡åº“è·¯å¾„",
            value="/home/fangyi1/Deepseeké¡¹ç›®/RAGé¡¹ç›®/medical_retrieval/MedSE2RAG/data/vector_database/0.6B/Definition_vector_store/", 
            help="ç—‡çŠ¶çŸ¥è¯†åº“çš„å‘é‡å­˜å‚¨è·¯å¾„"
        )
        
        # æ¨¡å‹é…ç½®
        embedding_model = st.text_input(
            "åµŒå…¥æ¨¡å‹è·¯å¾„",
            value="/mnt/fang/qwen3_embedding_0.6B_finetuned_inbatch_pro_earlystop",
            help="åµŒå…¥æ¨¡å‹çš„è·¯å¾„"
        )
        
        reranker_model = st.text_input(
            "é‡æ’åºæ¨¡å‹è·¯å¾„",
            value="Qwen/Qwen3-Reranker-0.6B",
            help="é‡æ’åºæ¨¡å‹çš„è·¯å¾„ï¼ˆæ”¯æŒHugging Faceæ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„ï¼‰"
        )
        
        # é«˜çº§é€‰é¡¹
        with st.expander("é«˜çº§é€‰é¡¹"):
            use_rerank = st.checkbox("å¯ç”¨é‡æ’åº", value=True)
            multi_shard = st.checkbox("ä½¿ç”¨å¤šåˆ†ç‰‡å‘é‡åº“", value=True)
            initial_k = st.slider("åˆå§‹æ£€ç´¢æ•°é‡", 10, 50, 20)
            final_k = st.slider("æœ€ç»ˆæ–‡æ¡£æ•°é‡", 3, 15, 5)
            model_name = st.text_input(
                "APIæ¨¡å‹åç§°",
                value="deepseek-chat" if api_type == "deepseek" else "",
                help="æŒ‡å®šè¦ä½¿ç”¨çš„å…·ä½“æ¨¡å‹åç§°"
            )
            task_type = st.selectbox(
                "ä»»åŠ¡ç±»å‹",
                ["symptom_extraction", "normal", "definition", "treatment"],
                help="é€‰æ‹©å¤„ç†ä»»åŠ¡çš„ç±»å‹"
            )
        
        # ç³»ç»Ÿåˆå§‹åŒ–
        if st.button("ğŸ”„ åˆå§‹åŒ–ç³»ç»Ÿ", type="primary"):
            config = Config(
                api_type=api_type,
                api_key=api_key,
                model=model_name,
                vector_path=vector_path,
                symptom_path=symptom_path,
                embedding_model=embedding_model,
                reranker_model=reranker_model,
                use_rerank=use_rerank,
                multi_shard=multi_shard,
                initial_k=initial_k,
                final_k=final_k
            )

            st.write("é…ç½®ä¿¡æ¯:")
            st.write(f"- å¤šåˆ†ç‰‡æ¨¡å¼: {multi_shard}")
            st.write(f"- ä¸»å‘é‡åº“è·¯å¾„: {vector_path}")
            st.write(f"- APIæ¨¡å‹: {model_name}")
            st.write(f"- å¯ç”¨é‡æ’åº: {use_rerank}")

            # æ£€æŸ¥åˆ†ç‰‡æ–‡ä»¶
            if os.path.exists(vector_path):
                files = os.listdir(vector_path)
                shard_files = [f for f in files if 'index_' in f and '.faiss' in f]
                st.write(f"- å‘ç°åˆ†ç‰‡æ–‡ä»¶: {len(shard_files)} ä¸ª")
                if shard_files:
                    st.write(f"- åˆ†ç‰‡æ–‡ä»¶ç¤ºä¾‹: {shard_files[:3]}")
            else:
                st.warning(f"âš ï¸ ä¸»å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨: {vector_path}")
            
            if initialize_system(config, multi_shard):
                st.success("âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
            else:
                st.error("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
    
    with tab1:  # ç—‡çŠ¶æŸ¥è¯¢æ ‡ç­¾é¡µ
        if st.session_state.initialized:
            st.header("ç—‡çŠ¶æŸ¥è¯¢")
            
            # è¾“å…¥åŒºåŸŸ
            col1, col2 = st.columns([3, 1])
            
            with col1:
                user_input = st.text_area(
                    "è¯·æè¿°æ‚¨çš„ç—‡çŠ¶æˆ–åŒ»å­¦é—®é¢˜ï¼š",
                    placeholder="ä¾‹å¦‚ï¼šæˆ‘æ„Ÿè§‰å¤´æ™•æ™•çš„ï¼Œå¾ˆä¸èˆ’æœ...",
                    height=100
                )
            
            with col2:
                question_type = st.selectbox(
                    "é—®é¢˜ç±»å‹",
                    ["ç—‡çŠ¶", "æ²»ç–—æ–¹æ³•", "è¯Šæ–­æ ‡å‡†", "é¢„é˜²æªæ–½", "å¹¶å‘ç—‡"]
                )
                
                if st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
                    if user_input.strip():
                        analyze_symptoms(user_input, question_type, task_type)
                    else:
                        st.warning("è¯·è¾“å…¥ç—‡çŠ¶æè¿°")
            
            # ç”¨æˆ·åé¦ˆåŒºåŸŸ
            if st.session_state.current_query_id:
                st.markdown("---")
                st.header("ğŸ“ ç»“æœåé¦ˆ")
                
                col1, col2 = st.columns([2, 1])
                with col1:
                    feedback_text = st.text_area(
                        "è¯·å¯¹æœ¬æ¬¡æŸ¥è¯¢ç»“æœè¿›è¡Œè¯¦ç»†è¯„ä»·ï¼š",
                        placeholder="è¯·è¯„ä»·ç»“æœçš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€æœ‰ç”¨æ€§ç­‰æ–¹é¢ï¼Œå¹¶æå‡ºæ”¹è¿›å»ºè®®...",
                        height=120
                    )
                    
                    # è¯¦ç»†åé¦ˆé€‰é¡¹
                    st.write("**è¯¦ç»†è¯„ä»·ç»´åº¦ï¼š**")
                    col1a, col1b = st.columns(2)
                    with col1a:
                        accuracy = st.selectbox("å‡†ç¡®æ€§", ["å¾ˆå‡†ç¡®", "è¾ƒå‡†ç¡®", "ä¸€èˆ¬", "ä¸å¤ªå‡†ç¡®", "å¾ˆä¸å‡†ç¡®"])
                        completeness = st.selectbox("å®Œæ•´æ€§", ["å¾ˆå®Œæ•´", "è¾ƒå®Œæ•´", "ä¸€èˆ¬", "ä¸å¤Ÿå®Œæ•´", "å¾ˆä¸å®Œæ•´"])
                    with col1b:
                        relevance = st.selectbox("ç›¸å…³æ€§", ["å¾ˆç›¸å…³", "è¾ƒç›¸å…³", "ä¸€èˆ¬", "ä¸å¤ªç›¸å…³", "å¾ˆä¸ç›¸å…³"])
                        usefulness = st.selectbox("æœ‰ç”¨æ€§", ["å¾ˆæœ‰ç”¨", "è¾ƒæœ‰ç”¨", "ä¸€èˆ¬", "ä¸å¤ªæœ‰ç”¨", "å¾ˆæ— ç”¨"])
                
                with col2:
                    rating = st.selectbox(
                        "æ•´ä½“æ»¡æ„åº¦è¯„åˆ†ï¼š",
                        [5, 4, 3, 2, 1],
                        format_func=lambda x: f"{x}åˆ† - {'éå¸¸æ»¡æ„' if x==5 else 'æ»¡æ„' if x==4 else 'ä¸€èˆ¬' if x==3 else 'ä¸æ»¡æ„' if x==2 else 'éå¸¸ä¸æ»¡æ„'}"
                    )
                    
                    if st.button("ğŸ’¾ æäº¤åé¦ˆ", type="secondary", use_container_width=True):
                        if feedback_text.strip():
                            detailed_feedback = {
                                "accuracy": accuracy,
                                "completeness": completeness,
                                "relevance": relevance,
                                "usefulness": usefulness
                            }
                            
                            success = save_user_feedback(
                                st.session_state.current_query_id, 
                                st.session_state.current_record_id,
                                feedback_text, 
                                rating, 
                                detailed_feedback
                            )
                            
                            if success:
                                st.success("âœ… åé¦ˆå·²ä¿å­˜å¹¶ç”¨äºç³»ç»Ÿå­¦ä¹ ä¼˜åŒ–ï¼Œæ„Ÿè°¢æ‚¨çš„å®è´µæ„è§ï¼")
                            else:
                                st.warning("âš ï¸ åé¦ˆä¿å­˜å¯èƒ½ä¸å®Œæ•´ï¼Œä½†å·²è®°å½•åˆ°æ•°æ®åº“")
                            
                            st.session_state.current_query_id = None
                            st.session_state.current_record_id = None
                            
                            # å¼ºåˆ¶æ¸…é™¤æ‰€æœ‰ç›¸å…³ç¼“å­˜ï¼Œä»¥ç¡®ä¿ç»Ÿè®¡æ•°æ®å®æ—¶æ›´æ–°
                            st.session_state['query_logs_cache_1000'] = None # æ¸…ç©ºå…·ä½“ç¼“å­˜
                            st.session_state['rl_data_cache'] = None # æ¸…ç©ºå¼ºåŒ–å­¦ä¹ æ•°æ®ç¼“å­˜
                            
                            time.sleep(0.5)  # çŸ­æš‚å»¶è¿Ÿç¡®ä¿æ•°æ®å†™å…¥å®Œæˆ
                            st.rerun()
                        else:
                            st.warning("è¯·è¾“å…¥åé¦ˆå†…å®¹")
        else:
            st.info("ğŸ‘ˆ è¯·å…ˆåœ¨ã€Œç³»ç»Ÿé…ç½®ã€æ ‡ç­¾é¡µä¸­é…ç½®å¹¶åˆå§‹åŒ–ç³»ç»Ÿ")
    
    with tab3:  # æŸ¥è¯¢è®°å½•æ ‡ç­¾é¡µ
        st.header("æŸ¥è¯¢è®°å½•ä¸ç»Ÿè®¡")
        
        # æ·»åŠ åˆ·æ–°æŒ‰é’®
        col_refresh, col_export = st.columns([1, 4])
        with col_refresh:
            if st.button("ğŸ”„ åˆ·æ–°æ•°æ®"):
                st.session_state['query_logs_cache_1000'] = None # æ¸…ç©ºå…·ä½“ç¼“å­˜
                st.session_state['rl_data_cache'] = None # æ¸…ç©ºå¼ºåŒ–å­¦ä¹ æ•°æ®ç¼“å­˜
                st.rerun()
        
        # ç»Ÿè®¡ä¿¡æ¯
        logs = get_query_logs(1000)  # è·å–æ›´å¤šæ•°æ®ç”¨äºç»Ÿè®¡
        if logs:
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric("æ€»æŸ¥è¯¢æ¬¡æ•°", len(logs))
            
            with col2:
                feedback_count = sum(1 for log in logs if log[6])  # user_feedbackä¸ä¸ºç©º
                st.metric("åé¦ˆæ•°é‡", feedback_count)
            
            with col3:
                ratings = [log[7] for log in logs if log[7]]  # feedback_ratingä¸ä¸ºç©º
                avg_rating = sum(ratings) / len(ratings) if ratings else 0
                st.metric("å¹³å‡è¯„åˆ†", f"{avg_rating:.1f}")
            
            with col4:
                successful_queries = sum(1 for log in logs if 'completed' in str(log[5]))
                success_rate = (successful_queries / len(logs) * 100) if logs else 0
                st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")
            
            with col5:
                if st.button("ğŸ“ å¯¼å‡ºExcel", type="secondary"):
                    try:
                        filename = export_to_excel()
                        if filename:
                            st.success(f"âœ… æŸ¥è¯¢è®°å½•å·²å¯¼å‡ºåˆ°: {filename}")
                            # æä¾›ä¸‹è½½é“¾æ¥
                            with open(filename, "rb") as file:
                                st.download_button(
                                    label="ğŸ“¥ ä¸‹è½½Excelæ–‡ä»¶",
                                    data=file,
                                    file_name=filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                                )
                        else:
                            st.warning("æš‚æ— æ•°æ®å¯å¯¼å‡º")
                    except Exception as e:
                        st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
        
        # è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰å±•å¼€ï¼‰
        with st.expander("ğŸ”§ è°ƒè¯•ä¿¡æ¯"):
            debug_logs = get_query_logs(5, force_refresh=True)
            st.write("æœ€è¿‘5æ¡è®°å½•çš„åé¦ˆçŠ¶æ€:")
            for i, log in enumerate(debug_logs):
                feedback_status = "æœ‰åé¦ˆ" if log[6] else "æ— åé¦ˆ"
                rating = log[7] if log[7] else "æ— è¯„åˆ†"
                st.write(f"{i+1}. ID:{log[0]} - {feedback_status} - è¯„åˆ†:{rating} - æ—¶é—´:{log[1][:19]}")
        
        # æŸ¥è¯¢è®°å½•è¡¨
        if logs:
            st.subheader("æœ€è¿‘æŸ¥è¯¢è®°å½•")
            
            # è½¬æ¢ä¸ºDataFrameä¾¿äºæ˜¾ç¤º
            df_logs = []
            for log in logs[:20]:  # åªæ˜¾ç¤ºæœ€è¿‘20æ¡
                df_logs.append({
                    "æ—¶é—´": log[1][:19],  # æˆªå–åˆ°ç§’
                    "æŸ¥è¯¢å†…å®¹": log[2][:50] + "..." if len(log[2]) > 50 else log[2],
                    "é—®é¢˜ç±»å‹": log[3],
                    "å¤„ç†æ—¶é—´(ç§’)": f"{log[4]:.2f}" if log[4] else "N/A",
                    "çŠ¶æ€": log[5],
                    "è¯„åˆ†": log[7] if log[7] else "æœªè¯„åˆ†",
                    "æœ‰åé¦ˆ": "æ˜¯" if log[6] else "å¦",
                    "æå–ç—‡çŠ¶": (log[9][:30] + "...") if len(log) > 9 and log[9] and len(log[9]) > 30 else (log[9] if len(log) > 9 else ""),
                    "ç—‡çŠ¶ä¸€è‡´æ€§": log[10] if len(log) > 10 and log[10] else "",
                    "è®°å½•ID": log[8][:8] + "..." if log[8] and log[8] != 'N/A' else "N/A"
                })
            
            df = pd.DataFrame(df_logs)
            st.dataframe(df, use_container_width=True)
        else:
            st.info("æš‚æ— æŸ¥è¯¢è®°å½•")
    
    with tab4:  # å¼ºåŒ–å­¦ä¹ æ•°æ®æ ‡ç­¾é¡µ
        st.header("å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®")
        
        # è·å–å¼ºåŒ–å­¦ä¹ æ•°æ®
        # å¼ºåˆ¶åˆ·æ–°ç¡®ä¿è·å–æœ€æ–°æ•°æ®
        recorder_for_rl = get_query_recorder_instance()
        rl_data = recorder_for_rl.get_rl_training_data()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("è®­ç»ƒæ ·æœ¬æ•°", len(rl_data))
        
        with col2:
            if rl_data:
                avg_reward = sum(r['reinforcement_learning']['reward_signal'] for r in rl_data) / len(rl_data)
                st.metric("å¹³å‡å¥–åŠ±ä¿¡å·", f"{avg_reward:.3f}")
            else:
                st.metric("å¹³å‡å¥–åŠ±ä¿¡å·", "0.000")
        
        with col3:
            if st.button("ğŸ“ å¯¼å‡ºRLæ•°æ®"):
                if rl_data:
                    count = recorder_for_rl.export_rl_data()
                    st.success(f"å·²å¯¼å‡º {count} æ¡å¼ºåŒ–å­¦ä¹ æ•°æ®åˆ° rl_training_data.json")
                else:
                    st.warning("æš‚æ— å¯å¯¼å‡ºçš„å¼ºåŒ–å­¦ä¹ æ•°æ®")
        
        if rl_data:
            # å¥–åŠ±ä¿¡å·è¶‹åŠ¿å›¾
            st.subheader("å¥–åŠ±ä¿¡å·è¶‹åŠ¿")
            reward_data = []
            for i, record in enumerate(rl_data[-50:]):  # æœ€è¿‘50æ¡
                reward_data.append({
                    "åºå·": i + 1,
                    "å¥–åŠ±ä¿¡å·": record['reinforcement_learning']['reward_signal'],
                    "è´¨é‡åˆ†æ•°": record['reinforcement_learning']['quality_score'],
                    "ç”¨æˆ·æ»¡æ„åº¦": record['reinforcement_learning']['user_satisfaction']
                })
            
            if reward_data:
                df_rewards = pd.DataFrame(reward_data)
                st.line_chart(df_rewards.set_index("åºå·"))
            
            # è¯¦ç»†æ•°æ®è¡¨
            st.subheader("è¯¦ç»†è®­ç»ƒæ•°æ®")
            table_data = []
            for record in rl_data[-15:]:  # æ˜¾ç¤ºæœ€è¿‘15æ¡
                table_data.append({
                    "æ—¶é—´": record['timestamp'][:19],
                    "æŸ¥è¯¢å†…å®¹": record['query_info']['user_input'][:40] + "...",
                    "ç”¨æˆ·è¯„åˆ†": record['user_feedback']['rating'],
                    "å¥–åŠ±ä¿¡å·": f"{record['reinforcement_learning']['reward_signal']:.3f}",
                    "è´¨é‡åˆ†æ•°": f"{record['reinforcement_learning']['quality_score']:.3f}",
                    "æ£€ç´¢æ•ˆæœ": f"{record['reinforcement_learning']['retrieval_effectiveness']:.3f}",
                    "ç”¨æˆ·æ»¡æ„åº¦": f"{record['reinforcement_learning']['user_satisfaction']:.3f}"
                })
            
            if table_data:
                df_rl = pd.DataFrame(table_data)
                st.dataframe(df_rl, use_container_width=True)
        else:
            st.info("æš‚æ— å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®ã€‚è¯·å…ˆè¿›è¡ŒæŸ¥è¯¢å¹¶æä¾›åé¦ˆä»¥ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚")

def analyze_symptoms(user_input, question_type, task_type):
    """åˆ†æç—‡çŠ¶"""
    with st.spinner('æ­£åœ¨åˆ†æï¼Œè¯·ç¨å€™...'):
        start_time = time.time()
        
        try:
            result = st.session_state.processor.process_single_query(
                term=user_input,
                question_type=question_type,
                query_id=1,
                task_type=task_type
            )
            
            processing_time = time.time() - start_time
            result['processing_time'] = processing_time
            
            # ä¿å­˜åˆ°JSONLæ–‡ä»¶
            recorder_instance = get_query_recorder_instance()
            record_id = recorder_instance.save_query_record(user_input, question_type, task_type, result, processing_time)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            query_id = save_query_log(user_input, question_type, task_type, result, processing_time, record_id)
            
            st.session_state.current_query_id = query_id
            st.session_state.current_record_id = record_id
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"âœ… åˆ†æå®Œæˆï¼è€—æ—¶ï¼š{processing_time:.2f}ç§’ (è®°å½•ID: {record_id[:12]}...)")
            display_result(result)
            
        except Exception as e:
            st.error(f"âŒ åˆ†æå¤±è´¥ï¼š{str(e)}")
            import traceback
            st.code(traceback.format_exc())

def display_result(result):
    """æ˜¾ç¤ºåˆ†æç»“æœ"""
    # æ€»ä½“çŠ¶æ€
    status_map = {
        "completed": "ğŸŸ¢ å®Œå…¨æˆåŠŸ",
        "completed_with_meta_eval": "ğŸŸ¡ å…ƒè¯„ä¼°å®Œæˆ", 
        "partially_completed": "ğŸŸ  éƒ¨åˆ†å®Œæˆ",
        "failed": "ğŸ”´ å¤±è´¥"
    }
    
    overall_status = result.get('overall_status', 'unknown')
    st.markdown(f"**çŠ¶æ€**: {status_map.get(overall_status, overall_status)}")
    
    # ç—‡çŠ¶æå–ç»“æœï¼ˆå¦‚æœæ˜¯ç—‡çŠ¶æå–ä»»åŠ¡ï¼‰
    if result.get('task_type') == 'symptom_extraction':
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "ç—‡çŠ¶ä¸€è‡´æ€§",
                result.get('is_symptom_consistent', 'N/A')
            )
        
        with col2:
            st.metric(
                "æå–ç—‡çŠ¶", 
                result.get('overall_pure_symptoms', 'N/A')
            )
        
        with col3:
            st.metric(
                "å¤„ç†æ—¶é—´",
                f"{result.get('total_processing_time', 0):.2f}s"
            )
    
    # è¯¦ç»†ç»“æœ
    st.subheader("è¯¦ç»†ç»“æœ")
    
    # ä¸»çŸ¥è¯†åº“ç»“æœ
    with st.expander("ğŸ¥ ä¸»çŸ¥è¯†åº“ç»“æœ", expanded=True):
        main_answer = result.get('main_kb_answer', 'æ— ç»“æœ')
        st.markdown(main_answer)
        
        if result.get('main_kb_has_reference') and result.get('main_kb_use_kb', False):
            st.caption(f"æ£€ç´¢åˆ° {result.get('main_kb_final_retrieved_count', 0)} ä¸ªç›¸å…³æ–‡æ¡£")
        else:
            st.caption("ä¸»çŸ¥è¯†åº“æœªå¤„ç†ï¼ˆçŸ¥è¯†åº“æœªåŠ è½½æˆ–é…ç½®ï¼‰")
    
    # ç—‡çŠ¶çŸ¥è¯†åº“ç»“æœ
    with st.expander("ğŸ©º ç—‡çŠ¶çŸ¥è¯†åº“ç»“æœ"):
        symptom_answer = result.get('symptom_kb_answer', 'æ— ç»“æœ')
        st.markdown(symptom_answer)
        
        if result.get('symptom_kb_has_reference') and result.get('symptom_kb_use_kb', False):
            st.caption(f"æ£€ç´¢åˆ° {result.get('symptom_kb_final_retrieved_count', 0)} ä¸ªç›¸å…³æ–‡æ¡£")
        else:
            st.caption("ç—‡çŠ¶çŸ¥è¯†åº“æœªå¤„ç†")
    
    # æœ€ç»ˆæ•´åˆç»“æœ
    with st.expander("ğŸ“‹ æ•´åˆç»“æœ", expanded=True):
        overall_answer = result.get('overall_answer', 'æ— æ•´åˆç»“æœ')
        st.markdown(overall_answer)
    
    # æŠ€æœ¯è¯¦æƒ…
    with st.expander("ğŸ”§ æŠ€æœ¯è¯¦æƒ…"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**ä¸»çŸ¥è¯†åº“**")
            st.write(f"- ä½¿ç”¨çŸ¥è¯†åº“: {result.get('main_kb_use_kb', False)}")
            st.write(f"- ä½¿ç”¨é‡æ’åº: {result.get('main_kb_use_rerank', False)}")
            st.write(f"- å¤„ç†æ—¶é—´: {result.get('main_kb_processing_time', 0):.2f}s")
            
        with col2:
            st.write("**ç—‡çŠ¶çŸ¥è¯†åº“**") 
            st.write(f"- ä½¿ç”¨çŸ¥è¯†åº“: {result.get('symptom_kb_use_kb', False)}")
            st.write(f"- ä½¿ç”¨é‡æ’åº: {result.get('symptom_kb_use_rerank', False)}")
            st.write(f"- å¤„ç†æ—¶é—´: {result.get('symptom_kb_processing_time', 0):.2f}s")

if __name__ == "__main__":
    main()
